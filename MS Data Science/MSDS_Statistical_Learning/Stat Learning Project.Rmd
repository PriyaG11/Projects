---
title: "Final Project"
subtitle: 'MATH 40028/50028: Statistical Learning'
date: "March 13, 2024"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
fontfamily: mathpazo
fontsize: 11pt
header-includes: \linespread{1.05}
urlcolor: blue
---

__ACADEMIC INTEGRITY: Every student should complete the project by their own. A project report having high degree of similarity with work by any other student, or with any other document (e.g., found online) is considered plagiarism, and will not be accepted. The minimal consequence is that the student will receive the project score of 0, and the best possible overall course grade will be D. Additional consequences are described at http://www.kent.edu/policyreg/administrative-policy-regarding-student-cheating-and-plagiarism and will be strictly enforced.__

## Instruction

__Goal:__ The goal of the final project is to apply the statistical learning methods discussed in this course to perform predictive analysis of real-life data. You will need to identify prediction problem(s), carry out necessary exploratory data analysis, perform predictive analysis using statistical learning methods, assess the performance, and communicate the results in a report. 

__Report:__ Use this Rmd file as a template. Edit the file by adding your project title in the YAML, and including necessary information in the four sections: (1) Introduction, (2) Statistical learning strategies and methods, (3) Predictive analysis and results, and (4) Conclusion. 

__Submission:__ Please submit your project report as a PDF file (8-10 pages, flexible) to Canvas by __11:59 p.m. on May 5, 2024__. The PDF file should be generated by “knitting” the Rmd file. You may choose to first generate an HTML file (by changing the output format in the YAML to `output: html_document`) and then convert it to PDF. Word documents, however, cannot be used as an intermediate file (and of course, the submitted file). __20 points will be deducted if the submitted files are in wrong format.__ 

__Grade:__ The project will be graded based on your ability to (1) recognize and define prediction problems, (2) identify potentially useful statistical learning methods, (3) perform the predictive analysis and assess the performance, (4) document the analysis procedure (with R code) and clearly present the results, and (5) draw valid conclusions supported by the analysis.

__Datasets:__ You may consider (but are not restricted) to use the following packages/datasets. 

* [`ISLR2`](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf): datasets used in the _Introduction to Statistical Learning_ textbook
* [`dslabs`](https://cran.r-project.org/web/packages/dslabs/dslabs.pdf)
* [UCI Machine Learning Repository](https://archive.ics.uci.edu/)

\pagebreak

## Introduction [15 points]

* Describe the dataset. What is the dataset about? 
* If possible, comment on the target population, sampling strategies, potential bias, etc. 
* Identify and define prediction problem(s).
* Discuss how to split the data into training and test sets among other plans for the use of data.

The dataset College from the ISLR2 package contains information about various attributes of American colleges and universities. It has observations of 777 colleges on 18 variables. It includes features such as the number of applications received (Apps), the number of students accepted (Accept), enrollment numbers (Enroll), tuition fees (Outstate), room and board costs (Room.Board), estimated books cost (Books) and other related variables. The target variable, Private, denotes whether the college is private or public. This dataset aims to explore factors influencing college characteristics and assess the predictive ability of these features on whether a college is private or public.

Regarding the target population, the dataset likely represents colleges and universities in the United States from the 1995 issue of US News and World Report. The sampling strategy is not explicitly mentioned in the dataset description, but it likely involves data collection from a variety of sources, such as educational institutions, surveys, or public records. Potential biases may exist due to the inclusion of certain types of colleges or geographical regions more prominently than others, as well as any limitations in the data collection process.

The prediction problem involves classifying colleges into private or public based on their attributes. This is a binary classification problem, where the goal is to develop a predictive model that can accurately classify colleges into these two categories (Private and Public) based on various features like number of applications received, accepted students, enrollment, etc.

To train and evaluate predictive models, the dataset is split into training and test sets using a stratified sampling approach to ensure that both sets maintain similar class distributions. In this case, approximately 80% of the data is allocated to the training set, while the remaining 20% is reserved for testing the model's performance. i.e., 614 observations for tarining set and 163 observations for testing set. This partitioning allows for the development of predictive models on the training data and the unbiased evaluation of their performance on unseen data from the test set. Additionally, cross-validation techniques may be employed during model development to further assess model generalization and performance.

```{r}
library(ISLR2)
data("College")
#head(College)
names(College)
#str(College)
#summary(College)

#checking missing values
colSums(is.na(College))

#classification problem
table(College$Private)

library(caret)
index = sample(2,nrow(College),replace = TRUE,prob = c(.80,.20))
train_set = College[index == 1,]
test_set = College[index == 2,]
#head(train_set)
#head(test_set)
dim(train_set)
dim(test_set)

library(ggplot2)
# plot the distribution of the target variable (Private)
ggplot(College, aes(x = Private)) +
  geom_bar(fill = "lightblue", color = "black") + 
  labs(title = "Distribution of Private Colleges",
       x = "Private",
       y = "Frequency")
```
The graph shows the distribution of the target variable (Private). The x-axis represents the variable Private and the y-axis represents the Frequency.  The blue bars show that there are more private colleges than public colleges in the dataset.

## Statistical learning strategies and methods [35 points]

* Perform exploratory data analysis using the training set.
```{r}
#summary(train_set)

#outliers
#boxplot(College)

#correlation
numerical_columns = sapply(College, is.numeric)
numerical_data = College[, numerical_columns]
correlation = cor(numerical_data)
#correlation
```
The correlation matrix shows the pairwise correlations between different variables from the dataset. Each cell in the matrix represents the correlation coefficient between two variables.
High positive correlation, i.e., between applications and acceptances (0.943), indicates they increase together. Low positive correlation, i.e., between applications and out-of-state tuition (0.050), suggests a weaker link. Negative correlations, i.e., student/faculty ratio and out-of-state tuition (-0.554), mean they move oppositely. Variables with correlations near zero, i.e., Books and Personal (0.179), show linear relationship.

```{r}
#scatter plot of Private Colleges vs. Number of applications received by the college
ggplot(train_set, aes(x = Apps, y = Private)) + 
  geom_point() +
  labs(title = "Private Colleges vs. Number of applications received by the college",
      x = "Number of applications recieved by the college", y = "Private Colleges")
```
The scatter plot depicts the relationship between whether a college is private or not (Yes or No on the y-axis) and the number of applications received by the college (on the x-axis). It suggests that private colleges generally tend to receive a higher number of applications compared to public colleges, as evidenced by the dots representing private colleges being more concentrated towards the higher end of the x-axis. However, there is a wide range and considerable overlap in the number of applications received by both private and public colleges, indicating that while the private status may contribute to a higher number of applications on average, other factors also play a role, and the distinction is not absolute.

```{r}
#scatter plot of Private Colleges vs. Number of applications accepted by the college
ggplot(train_set, aes(x = Accept, y = Private)) +
 geom_point()+
 labs(title = "Private Colleges vs. Number of applications accepted by the college",
 x = "Number of applications accepted by the college", y = "Private Colleges")
```
The scatter plot suggests the relationship between whether a college is private or not (Yes or No on the y-axis) and the number of applications accepted by the college (on the x-axis). It suggests that private colleges generally tend to accept fewer applications compared to public colleges, as most of the dots represent private colleges are concentrated towards the lower end of the x-axis. However, there is still a range in the number of applications accepted by both private and public colleges, indicating that other factors beyond the private status also influence the admission selectivity of a college.

```{r}
#scatter plot of Private Colleges vs. Number of students enrolled in the college
ggplot(train_set, aes(x = Enroll, y = Private)) +
 geom_point()+
 labs(title = "Private Colleges vs. Number of students enrolled in the college",
 x ="Number of students enrolled in the college", y = "Private Colleges")
```
The scatter plot suggests the relationship between whether a college is private or not (Yes or No on the y-axis) and the number of students enrolled in the college (on the x-axis). It suggests that public colleges tend to have a larger student enrollment compared to private colleges. The dots representing public colleges are more spread out and extend further toward the higher end of the x-axis, indicating a wider range of enrollment numbers, with some public colleges having significantly larger enrollments. In contrast, the dots for private colleges are more concentrated toward the lower end of the x-axis, suggest smaller student enrollments on average for private colleges.

* Describe the statistical learning approaches and other strategies for feature engineering (transformation, selection, etc.).
```{r}
#feature engineering: Transformation (log transformation) for train_set
train_set$log_Apps = log(train_set$Apps)
train_set$log_Accept = log(train_set$Accept)
train_set$log_Enroll = log(train_set$Enroll)
 
#scatter plot of Private vs. Log Number of applications received
ggplot(train_set, aes(x = log_Apps, y = Private)) +
 geom_point()+
 labs(title = "Private vs. Log Number of applications received",
 x = "Log Apps", y = "Private")
```
The scatter plot suggests the relationship between whether a college is private or not (Yes or No on the y-axis) and the logarithm of the number of applications received by the college (on the x-axis). It suggests that private colleges tend to receive a higher number of applications compared to public colleges, by the dots represent private colleges being more concentrated towards the higher end of the logarithmic scale on the x-axis. However, there is a considerable range and overlap in the number of applications received by both private and public colleges, indicating that while private status is generally associated with more applications on average.

```{r}
#feature engineering: Interaction terms for train_set
train_set$interaction1 = train_set$Apps * train_set$Accept 
train_set$interaction2 = train_set$Apps * train_set$Enroll
train_set$interaction3 = train_set$Accept * train_set$Enroll

#feature engineering: Transformation (log transformation) for test_set
test_set$log_Apps = log(test_set$Apps)
test_set$log_Accept = log(test_set$Accept)
test_set$log_Enroll = log(test_set$Enroll)
#feature engineering: Interaction terms for test_set
test_set$interaction1 = test_set$Apps * test_set$Accept 
test_set$interaction2 = test_set$Apps * test_set$Enroll
test_set$interaction3 = test_set$Accept * test_set$Enroll
```
In the process of feature engineering, logarithmic transformations are applied to certain variables (Apps, Accept, Enroll) to address skewness or non-normality in their distributions. Additionally, interaction terms are created by multiplying pairs of variables (i.e., Apps * Accept, Apps * Enroll, Accept * Enroll) to capture potential combined effects on the target variable.

```{r}
#feature selection: LASSO regression
#fit the model
lasso_reg = train(Private ~ log_Apps + log_Accept + log_Enroll + interaction1 + 
                    interaction2 + interaction3, data = train_set,
                  method = "glmnet", tuneLength = 10)
#train linear regression model with selected features
train_lasso = train_set[, c("Private", "log_Apps", "log_Accept", "log_Enroll", 
                            "interaction1", "interaction2", "interaction3")]
#scatter plot of Private vs. Log Number of applications received by the college (LASSO Selected Features)
ggplot(train_lasso, aes(x = log_Apps, y = Private)) +
  geom_point() +
  labs(title = "Private vs. Log Number of applications recieved by the college 
       (LASSO Selected Features)",
       x = "Log Apps", y = "Private Colleges")
#lasso_reg$results
```
For feature selection, LASSO regression is used to identify the most relevant predictors among the transformed variables and interaction terms. By leveraging regularization techniques, LASSO aids in selecting a subset of features that contribute significantly to predicting the target variable(Private). The LASSO regression results summarize the model's performance across various tuning parameter combinations. Each row represents a combination of alpha and lambda, with columns showing accuracy, kappa statistic, and their respective standard deviations. This helps choose the optimal tuning parameters that balance model performance and over fitting control.

* Based on the conditions assumed by the statistical learning methods, discuss their applicability to the prediction problem.

The analysis employs various techniques to enhance prediction accuracy. Log-transformations enhance the linearity between features and the target variable, aiding linear models. Interaction terms capture potential joint effects of features, providing richer information. LASSO regression combats the "curse of dimensionality" by selecting the most relevant features, boosting interpretability and potentially reducing overfitting. Logistic regression, suitable for binary classification, like predicting college types (Private vs. Public) based on the provided features. However, its assumptions like linearity and lack of multicollinearity need validation for reliable predictions.

```{r}
#check levels of the factor variable
levels(College$Private)
#fit logistic regression model
logistic_reg = glm(Private ~ log_Apps + log_Accept + log_Enroll + interaction1 + 
                     interaction2 + interaction3, data = train_set, 
                   family = binomial)
summary(logistic_reg)
```
Why logistic regression is used instead of linear regression: 

In scenarios where the response variable (Private) is binary or categorical, logistic regression is more suitable over linear regression due to its compatibility with the nature of the response variable and the assumptions it makes about the relationship between predictors and the response. It is tailored to model the probability of a binary outcome, allowing for a more flexible relationship between predictors and the outcome. The coefficients represent the change in the log odds of the outcome for a one-unit change in the predictor variable, facilitating a more intuitive understanding of the relationship between predictors and the binary outcome.

## Predictive analysis and results [35 points]

* Apply and document the statistical learning procedure for the predictive analysis.
* Estimate the performance of the statistical learning approaches on test data, using resampling methods or other measures.
* Evaluate the performance on the test data.
* Discuss the results.

```{r}
set.seed(1)
```

```{r}
#fit logistic regression
LR = glm(Private ~ ., data = train_set, family = binomial)
summary(LR)
#plot(LR, col = "yellow")
predict_LR = predict(LR,newdata = test_set, type = "response")
predict_LR = ifelse(predict_LR > 0.5,"Yes","No")
#table(predict_LR)
predict_LR = factor(predict_LR, levels = levels(test_set$Private))
#head(predict_LR)
#levels(test_set$Private)
#levels(predict_LR)
#str(predict_LR)
##confusion matrix, accuracy and performance
accuracy_LR = confusionMatrix(test_set$Private, predict_LR)
accuracy_LR
```
```{r}
#cross validation - logistic regression
num_folds = 10
accuracy_rates = numeric(num_folds)
folds = cut(seq(1, nrow(train_set)), breaks = num_folds, labels = FALSE)
for (i in 1:num_folds) {
  validation_indices = which(folds == i)
  validation_set = train_set[validation_indices, ]
  training_set = train_set[-validation_indices, ]
  lr = glm(Private ~ ., data = training_set, family = binomial)
  predicted_probs = predict(lr, newdata = validation_set, type = "response")
  predicted_classes = ifelse(predicted_probs > 0.5, "Yes", "No")
  accuracy_rates[i] = mean(predicted_classes == validation_set$Private)
}
mean_accuracy = mean(accuracy_rates)
mean_accuracy
```

```{r}
#random forest
library(randomForest)
library(caTools)
RF = randomForest(Private ~ ., data = train_set, ntree=500)
RF
#plot(RF)
#predicting test set data
predict_RF = predict(RF,newdata=test_set)
summary(predict_RF)
#confusion matrix, accuracy and performance
accuracy_RF = confusionMatrix(test_set$Private,predict_RF)
accuracy_RF
#feature importance of the random forest model
importance = RF$importance
#importance
```
The "MeanDecreaseGini" values provide insights into the importance of each predictor variable in the Random Forest model. Higher values indicate greater importance in predicting the outcome variable (in this case, "Private"). These values can help identify which predictors are most relevant for predicting the outcome and guide feature selection or further analysis.

```{r}
#cross validation - random forest
num_folds = 10
accuracy_rates = numeric(num_folds)
folds = cut(seq(1, nrow(train_set)), breaks = num_folds, labels = FALSE)
for (i in 1:num_folds) {
  validation_indices = which(folds == i)
  validation_set = train_set[validation_indices, ]
  training_set = train_set[-validation_indices, ]
  rf = randomForest(Private ~ ., data = training_set, ntree = 500)
  predicted_classes = predict(rf, newdata = validation_set)
  accuracy_rates[i] = mean(predicted_classes == validation_set$Private)
}
mean_accuracy = mean(accuracy_rates)
mean_accuracy
```

```{r}
#decision tree
library(party)
library(rpart)
library(rpart.plot)
dtree = rpart(Private ~ ., data = train_set)
dtree
#summary(dtree)
#rpart.plot(dtree)
#predicting test set data
predict_DT = predict(dtree, newdata=test_set, type = "class")
#summary(predict_DT)
##confusion matrix, accuracy and performance
accuracy_DT = confusionMatrix(test_set$Private,predict_DT)
accuracy_DT
```

```{r}
#cross validation - decision tree
num_folds = 10
accuracy_rates = numeric(num_folds)
folds = cut(seq(1, nrow(train_set)), breaks = num_folds, labels = FALSE)
for (i in 1:num_folds) {
  validation_indices = which(folds == i)
  validation_set = train_set[validation_indices, ]
  training_set = train_set[-validation_indices, ]
  dt = rpart(Private ~ ., data = training_set)
  predicted_classes = predict(dt, newdata = validation_set, type = "class")
  accuracy_rates[i] = mean(predicted_classes == validation_set$Private)
}
mean_accuracy = mean(accuracy_rates)
mean_accuracy
```

```{r}
#support vector machine
library(e1071)
SVM = svm(Private ~ ., data = train_set, kernel = "radial")
SVM
summary(SVM)
#predicting test set data
predict_SVM = predict(SVM, newdata  =test_set)
summary(predict_SVM)
##confusion matrix, accuracy and performance
accuracy_SVM = confusionMatrix(test_set$Private,predict_SVM)
accuracy_SVM
```

```{r}
#cross validation - support vector machine
num_folds = 10
accuracy_rates = numeric(num_folds)
folds = cut(seq(1, nrow(train_set)), breaks = num_folds, labels = FALSE)
for (i in 1:num_folds) {
  validation_indices <=which(folds == i)
  validation_set = train_set[validation_indices, ]
  training_set = train_set[-validation_indices, ]
  svm = svm(Private ~ ., data = training_set, kernel = "radial")
  predicted_classes = predict(svm, newdata = validation_set)
  accuracy_rates[i] = mean(predicted_classes == validation_set$Private)
}
mean_accuracy = mean(accuracy_rates)
mean_accuracy
```

```{r}
#discriminant analysis
library(MASS)
library(psych)
library(lda)
LDA = lda(Private ~ ., data = train_set)
#LDA
summary(LDA)
#plot(LDA,type = "density")
#pairs.panels(College[,1:12],gap = 0,bg = c("red","blue","green")[College$Private],ps = 21)
#predicting test set data
predict_LDA = predict(LDA, test_set)$class
summary(predict_LDA)
##confusion matrix, accuracy and performance
accuracy_LDA = confusionMatrix(test_set$Private,predict_LDA)
accuracy_LDA
```

```{r}
#cross validation - discriminant analysis
num_folds = 10
accuracy_rates = numeric(num_folds)
folds = cut(seq(1, nrow(train_set)), breaks = num_folds, labels = FALSE)
for (i in 1:num_folds) {
  validation_indices = which(folds == i)
  validation_set = train_set[validation_indices, ]
  training_set = train_set[-validation_indices, ]
  discriminant = lda(Private ~ ., data = training_set)
  predicted_classes = predict(discriminant, newdata = validation_set)$class
  accuracy_rates[i] = mean(predicted_classes == validation_set$Private)
}
mean_accuracy = mean(accuracy_rates)
mean_accuracy
```
NOTE: Accuracies are considered approximately as the values are changing while I knit the file.

RESULTS:

All models achieved high accuracies on the test data, with logistic regression, random forest, and linear discriminant analysis performing particularly well. Logistic regression and random forest both attained the highest accuracy of 94.19%, followed closely by linear discriminant analysis with an accuracy of 95.48%.

When considering cross-validated accuracy, which provides an estimate of how well the models are likely to perform on unseen data, random forest achieved the highest cross-validated accuracy of 93.73%. This suggests that random forest may have slightly better generalization performance compared to the other models. However, logistic regression and linear discriminant analysis also demonstrated strong cross-validated accuracies of 92.60% and 92.92%, respectively.

Decision trees and support vector machines also performed well, with accuracies of 90.97% and 93.55%, respectively. While decision trees had a slightly lower accuracy compared to the other models, it still provided a good performance.

Overall, the results indicate that all models were effective in predicting the target variable on the test data. The choice of the best model may depend on factors such as interpretability, computational efficiency, and the specific requirements of the problem at hand.

Additionally, confusion matrices and other class-specific performance metrics such as precision, recall, F1 score etc, were also calculated for each model. These metrics provide further insights into the performance of the models, particularly in binary classification tasks where class imbalance may exist or where different types of errors (false positives and false negatives) have varying costs or implications.  It allows to assess not only the overall accuracy but also the model's ability to correctly identify positive instances (precision) and capture all positive instances (recall) simultaneously.

## Conclusion [15 points]

* Discuss the scope and generalizability of the predictive analysis. 
* Discuss potential limitations and possibilities for improvement.

In conclusion, the predictive analysis conducted on the College dataset provides valuable insights into the factors influencing private college enrollment. The models employed, including logistic regression, random forest, decision trees, support vector machine, and linear discriminant analysis, demonstrated promising performance with high accuracy and relatively consistent cross-validated accuracy. The scope encompasses while the models performed well on the test data, their applicability to other datasets or real-world scenarios may vary. Factors such as dataset characteristics, sampling bias, and the specific context of the problem may influence the model's performance in different settings. This limits the generalizability of the findings to the specific time period and potentially excludes certain types of institutions. The imbalanced nature of the data, with significantly more public colleges, could lead to models biased towards the majority class, potentially affecting prediction accuracy for private colleges. To improve the predictive analysis, several possibilities can be explored. This includes collecting additional data to enhance the model's robustness and presentation, experimenting with different feature engineering techniques, such as polynomial features or feature scaling, and considering more advanced modeling approaches, such as ensemble methods or neural networks. Moreover, conducting further research to understand the underlying factors driving college enrollment decisions can provide valuable insights for refining the predictive models and addressing potential limitations.